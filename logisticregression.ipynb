{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading in all the import statements\n",
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the Dataset\n",
    "\n",
    "Using the breast cancer dataset that is available in the sklearn library for processing - The explanation of dataset can be found here : [Reference link :https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)].\n",
    "\n",
    "Also setting learning rate as 0.0002 and number of epocs at 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading the dataset - The data set is a breast_cancer data set that is read from the sklearn library\n",
    "cancerdata = datasets.load_breast_cancer()\n",
    "\n",
    "\n",
    "#print(diabetes.data)\n",
    "#print(diabetes.target)\n",
    "#print(diabetes)\n",
    "\n",
    "\n",
    "#splitting the feature matrix as X and target matrix as Y\n",
    "X = cancerdata.data\n",
    "y = cancerdata.target\n",
    "\n",
    "#setting the learning rate\n",
    "initialLearningRate = 0.0002\n",
    "\n",
    "#setting the epocs value to 150\n",
    "epocs = 150"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting dataset and normalizing the values to be between 0 and 1:\n",
    "\n",
    "The dataset is split into X_train, X_test, y_train, y_test and the X_train dataset is normalized to include only values between 0 and 1 for us to computer logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using train test split to get values of x_train,x_test,y_train,y_test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=325) #[ Reference Link :https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html]\n",
    "\n",
    "\n",
    "\n",
    "#finding min and maximum values of each column in diabetes dataset\n",
    "#[Refernce link : https://machinelearningmastery.com/scale-machine-learning-data-scratch-python/]\n",
    "iterator = 0\n",
    "minmax = []\n",
    "while(iterator < 30): #since the length of X_train[0] = 30\n",
    "    colvals = [] #holds all the column values\n",
    "    for val in X_train:\n",
    "        colvals.append(val[iterator])\n",
    "    max1 = max(colvals) #max value of each column\n",
    "    min1 = min(colvals) # min value of each column\n",
    "    listcreated = []\n",
    "    listcreated.append([min1,max1]) #appends min and max values of each column data\n",
    "    minmax.append(listcreated[0])\n",
    "    iterator = iterator + 1\n",
    "\n",
    "#normalizing data set to hold values between 0 and 1\n",
    "#[Refernce link : https://machinelearningmastery.com/scale-machine-learning-data-scratch-python/]\n",
    "for everyval in X_train:\n",
    "    iterator = 0\n",
    "    lengthreq = len(everyval)\n",
    "    while(iterator < lengthreq):\n",
    "        firstsub = everyval[iterator] - minmax[iterator][0] #first sbutraction in fromula for normalizing\n",
    "        secondsub = minmax[iterator][1] - minmax[iterator][0] #second sbutraction in fromula for normalizing\n",
    "        everyval[iterator] = (firstsub) / (secondsub) #dividing both values\n",
    "        iterator = iterator + 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initilize weight array and running the algorithm:\n",
    "\n",
    "The stochastic gradient descent function is run from the update function which holds the formula - > stochastic gradient descent final cal(weight) - > widght - lr * pred xi - act yi * xi , it runs for all the epocs set on hyper parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initilizing weights array with all 0's to start with\n",
    "weights = [] #creating a weights array\n",
    "iterator = 0\n",
    "while(iterator < 31): #since the X.train[1] is 30\n",
    "    weights.append(0)\n",
    "    iterator = iterator + 1\n",
    " \n",
    "#function to find exponent\n",
    "def findexpo(x):\n",
    "    return math.exp(x)\n",
    "\n",
    "#stochastic gradient descent final cal - > widght - lr * pred xi - act yi * xi \n",
    "def update(weights,x,y):\n",
    "    mul1 = np.append(x,1) #using np.append to add 1 to the created array [Refernce link : https://docs.scipy.org/doc/numpy/reference/generated/numpy.append.html]\n",
    "    predicted_y = weights @ mul1 #dot product function  [reference link : https://stackoverflow.com/questions/5919530/what-is-the-pythonic-way-to-calculate-dot-product]\n",
    "    predicted_y = 1/(1+findexpo(-predicted_y)) #sigmoid function [https://kite.com/python/answers/how-to-calculate-a-logistic-sigmoid-function-in-python]\n",
    "    sub1 = predicted_y - y\n",
    "    weights = weights - initialLearningRate * sub1 * mul1 #stochastic gradient formula\n",
    "    return weights\n",
    "\n",
    "\n",
    "#running for all epocs\n",
    "iterator = 0\n",
    "while(iterator < epocs): #runs through all epocs\n",
    "    iterator1 = 0\n",
    "    while(iterator1 < 381): #since the X.train[0] is 381\n",
    "        weights = update(weights,X_train[iterator,:],y_train[iterator]) #calling the update function to update all wight values\n",
    "        iterator1 = iterator1 + 1\n",
    "    iterator = iterator + 1\n",
    "\n",
    "#print(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizing the X_test dataset to hold values between 0 and 1\n",
    "\n",
    "This part of the code now normalizes the values of X_test dataset to hold valued between 0 and 1 for all the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding min and maximum values of each column in diabetes test dataset\n",
    "#[Refernce link : https://machinelearningmastery.com/scale-machine-learning-data-scratch-python/]\n",
    "iterator = 0\n",
    "minmax = []\n",
    "while(iterator < 30):\n",
    "    colvals = [] #holds all column values\n",
    "    for val in X_test:\n",
    "        colvals.append(val[iterator])\n",
    "    max1 = max(colvals) #max value of each column\n",
    "    min1 = min(colvals) # min value of each column\n",
    "    listcreated = []\n",
    "    listcreated.append([min1,max1]) #holds all min and max values for each column\n",
    "    minmax.append(listcreated[0]) \n",
    "    iterator = iterator + 1\n",
    "\n",
    "    \n",
    "#normalizing data set to hold values between 0 and 1\n",
    "\n",
    "for everyval in X_test:\n",
    "    iterator = 0\n",
    "    lengthreq = len(everyval)\n",
    "    while(iterator < lengthreq):\n",
    "        firstsub = everyval[iterator] - minmax[iterator][0]\n",
    "        secondsub = minmax[iterator][1] - minmax[iterator][0]\n",
    "        everyval[iterator] = (firstsub) / (secondsub)\n",
    "        iterator = iterator + 1  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting the accuracy\n",
    "\n",
    "Here we try to predict the accuracy from the comparisions performed on _test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of the logistic regression with SGD is =  93.08510638297872 %\n"
     ]
    }
   ],
   "source": [
    "predicts = []\n",
    "for everyval in X_test:\n",
    "    mul1 = np.append(everyval,1)\n",
    "    predicted = weights @ mul1 #dot product function  [reference link : https://stackoverflow.com/questions/5919530/what-is-the-pythonic-way-to-calculate-dot-product]\n",
    "    predicted = 1/(1+findexpo(-predicted)) #sigmoid function [reference link : https://kite.com/python/answers/how-to-calculate-a-logistic-sigmoid-function-in-python]\n",
    "    predicts.append(predicted)\n",
    "   \n",
    "#rounding each value to be exactly 0 or 1\n",
    "iterator =0\n",
    "while(iterator < len(predicts)):\n",
    "    predicts[iterator] = round(predicts[iterator])\n",
    "    iterator = iterator + 1\n",
    "\n",
    "\n",
    "accuracylist = []\n",
    "iterator = 0\n",
    "lenreq = len(y_test)\n",
    "while(iterator < lenreq):\n",
    "    if(y_test[iterator] == predicts[iterator]):\n",
    "        accuracylist.append(1)\n",
    "    iterator = iterator + 1\n",
    "    \n",
    "accuracy = sum(accuracylist)/len(y_test) \n",
    "print(\"accuracy of the logistic regression with SGD is = \",accuracy*100,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
